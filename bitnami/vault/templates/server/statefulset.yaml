{{- /*
Copyright Broadcom, Inc. All Rights Reserved.
SPDX-License-Identifier: APACHE-2.0
*/}}

{{- if .Values.server.enabled }}
apiVersion: {{ include "common.capabilities.statefulset.apiVersion" . }}
kind: StatefulSet
metadata:
  name: {{ template "vault.server.fullname" . }}
  namespace: {{ include "common.names.namespace" . | quote }}
  labels: {{- include "common.labels.standard" ( dict "customLabels" .Values.commonLabels "context" $ ) | nindent 4 }}
    app.kubernetes.io/part-of: vault
    app.kubernetes.io/component: server
  {{- if .Values.commonAnnotations }}
  annotations: {{- include "common.tplvalues.render" ( dict "value" .Values.commonAnnotations "context" $ ) | nindent 4 }}
  {{- end }}
spec:
  replicas: {{ .Values.server.replicaCount }}
  serviceName: {{ printf "%s-headless" (include "vault.server.fullname" .) }}
  podManagementPolicy: {{ .Values.server.podManagementPolicy }}
  {{- if .Values.server.updateStrategy }}
  updateStrategy: {{- toYaml .Values.server.updateStrategy | nindent 4 }}
  {{- end }}
  {{- $podLabels := include "common.tplvalues.merge" ( dict "values" ( list .Values.server.podLabels .Values.commonLabels ) "context" . ) }}
  selector:
    matchLabels: {{- include "common.labels.matchLabels" ( dict "customLabels" $podLabels "context" $ ) | nindent 6 }}
      app.kubernetes.io/part-of: vault
      app.kubernetes.io/component: server
  template:
    metadata:
      {{- if .Values.server.podAnnotations }}
      annotations: {{- include "common.tplvalues.render" (dict "value" .Values.server.podAnnotations "context" $) | nindent 8 }}
      {{- end }}
      labels: {{- include "common.labels.standard" ( dict "customLabels" $podLabels "context" $ ) | nindent 8 }}
        app.kubernetes.io/part-of: vault
        app.kubernetes.io/component: server
    spec:
      serviceAccountName: {{ template "vault.server.serviceAccountName" . }}
      {{- include "vault.imagePullSecrets" . | nindent 6 }}
      automountServiceAccountToken: {{ .Values.server.automountServiceAccountToken }}
      {{- if .Values.server.hostAliases }}
      hostAliases: {{- include "common.tplvalues.render" (dict "value" .Values.server.hostAliases "context" $) | nindent 8 }}
      {{- end }}
      {{- if .Values.server.affinity }}
      affinity: {{- include "common.tplvalues.render" ( dict "value" .Values.server.affinity "context" $) | nindent 8 }}
      {{- else }}
      affinity:
        podAffinity: {{- include "common.affinities.pods" (dict "type" .Values.server.podAffinityPreset "component" "server" "customLabels" $podLabels "context" $) | nindent 10 }}
        podAntiAffinity: {{- include "common.affinities.pods" (dict "type" .Values.server.podAntiAffinityPreset "component" "server" "customLabels" $podLabels  "context" $) | nindent 10 }}
        nodeAffinity: {{- include "common.affinities.nodes" (dict "type" .Values.server.nodeAffinityPreset.type "key" .Values.server.nodeAffinityPreset.key "values" .Values.server.nodeAffinityPreset.values) | nindent 10 }}
      {{- end }}
      {{- if .Values.server.nodeSelector }}
      nodeSelector: {{- include "common.tplvalues.render" ( dict "value" .Values.server.nodeSelector "context" $) | nindent 8 }}
      {{- end }}
      {{- if .Values.server.tolerations }}
      tolerations: {{- include "common.tplvalues.render" (dict "value" .Values.server.tolerations "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.server.priorityClassName }}
      priorityClassName: {{ .Values.server.priorityClassName | quote }}
      {{- end }}
      {{- if .Values.server.schedulerName }}
      schedulerName: {{ .Values.server.schedulerName | quote }}
      {{- end }}
      {{- if .Values.server.topologySpreadConstraints }}
      topologySpreadConstraints: {{- include "common.tplvalues.render" (dict "value" .Values.server.topologySpreadConstraints "context" .) | nindent 8 }}
      {{- end }}
      {{- if .Values.server.podSecurityContext.enabled }}
      securityContext: {{- include "common.compatibility.renderSecurityContext" (dict "secContext" .Values.server.podSecurityContext "context" $) | nindent 8 }}
      {{- end }}
      {{- if .Values.server.terminationGracePeriodSeconds }}
      terminationGracePeriodSeconds: {{ .Values.server.terminationGracePeriodSeconds }}
      {{- end }}
      initContainers:
        {{- if and .Values.volumePermissions.enabled .Values.server.containerSecurityContext.enabled }}
        - name: volume-permissions
          image: {{ include "vault.volumePermissions.image" . }}
          imagePullPolicy: {{ .Values.volumePermissions.image.pullPolicy | quote }}
          command:
            - sh
            - -c
            - |
              mkdir -p {{ .Values.server.persistence.mountPath }}
              find {{ .Values.server.persistence.mountPath }} -mindepth 1 -maxdepth 1 -not -name ".snapshot" -not -name "lost+found" | xargs --no-run-if-empty chown -R {{ .Values.server.containerSecurityContext.runAsUser }}:{{ .Values.server.podSecurityContext.fsGroup }}
          {{- if .Values.volumePermissions.containerSecurityContext.enabled }}
          securityContext: {{- include "common.compatibility.renderSecurityContext" (dict "secContext" .Values.volumePermissions.containerSecurityContext "context" $) | nindent 12 }}
          {{- end }}
          {{- if .Values.volumePermissions.resources }}
          resources: {{- toYaml .Values.volumePermissions.resources | nindent 12 }}
          {{- else if ne .Values.volumePermissions.resourcesPreset "none" }}
          resources: {{- include "common.resources.preset" (dict "type" .Values.volumePermissions.resourcesPreset) | nindent 12 }}
          {{- end }}
          volumeMounts:
            {{- if .Values.server.persistence.enabled }}
            - name: data
              mountPath: {{ .Values.server.persistence.mountPath }}
            {{- end }}
        {{- end }}
        {{- if .Values.server.initContainers }}
          {{- include "common.tplvalues.render" (dict "value" .Values.server.initContainers "context" $) | nindent 8 }}
        {{- end }}
        {{- if and .Values.restore.enabled .Values.backup.enabled }}
        ## Auto Restore Init Container
        ## Downloads latest snapshot from S3 if data directory is empty
        - name: auto-restore
          image: {{ .Values.backup.rclone.image.repository }}:{{ .Values.backup.rclone.image.tag }}
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -ec
            - |
              echo "=== Vault Auto Restore Init Container ==="
              echo "Restore mode: {{ .Values.restore.mode | default "auto" }}"
              echo "Cluster name: {{ .Values.backup.clusterName | default (include "common.names.fullname" .) }}"
              
              RESTORE_MODE="{{ .Values.restore.mode | default "auto" }}"
              
              # Check restore mode
              if [ "$RESTORE_MODE" = "never" ]; then
                echo "Restore mode is 'never', skipping restore"
                exit 0
              fi
              
              # Check if data directory has data
              DATA_EXISTS="false"
              if [ -d "{{ .Values.server.persistence.mountPath }}/raft" ] && [ "$(ls -A {{ .Values.server.persistence.mountPath }}/raft 2>/dev/null)" ]; then
                DATA_EXISTS="true"
                echo "Raft data directory exists and is not empty"
              else
                echo "Raft data directory is empty"
              fi
              
              # Decide whether to restore based on mode
              if [ "$DATA_EXISTS" = "true" ] && [ "$RESTORE_MODE" != "force" ]; then
                echo "Data exists and mode is not 'force', skipping restore"
                exit 0
              fi
              
              if [ "$DATA_EXISTS" = "true" ] && [ "$RESTORE_MODE" = "force" ]; then
                echo "WARNING: Force mode enabled, will overwrite existing data!"
              fi
              
              echo "Checking S3 for snapshots..."
              
              # Use cluster-isolated path
              REMOTE_PATH="{{ include "vault.backup.remotePath" . }}"
              REMOTE_FILE="${REMOTE_PATH}/latest.snap"
              
              echo "Remote path: $REMOTE_FILE"
              
              # Check if file exists using rclone ls
              if rclone ls "$REMOTE_FILE" --config /config/rclone.conf 2>/dev/null | grep -q "latest.snap"; then
                echo "Found latest snapshot in S3, downloading..."
                rclone copyto "$REMOTE_FILE" /vault/restore/restore.snap \
                  --config /config/rclone.conf \
                  --progress \
                  --retries 5 \
                  --retries-sleep 10s \
                  --low-level-retries 10 \
                  -v
                
                if [ -f /vault/restore/restore.snap ]; then
                  echo "Snapshot downloaded successfully"
                  ls -lh /vault/restore/restore.snap
                else
                  echo "WARNING: Failed to download snapshot"
                fi
              else
                echo "No snapshot found in S3 at: $REMOTE_FILE"
                echo "Starting fresh (this is expected for new deployments)"
              fi
          env:
            {{- if .Values.backup.rclone.credentialsSecretName }}
            - name: RCLONE_CONFIG_S3_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.backup.rclone.credentialsSecretName }}
                  key: {{ .Values.backup.rclone.accessKeySecretKey | default "AWS_ACCESS_KEY_ID" }}
            - name: RCLONE_CONFIG_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.backup.rclone.credentialsSecretName }}
                  key: {{ .Values.backup.rclone.secretKeySecretKey | default "AWS_SECRET_ACCESS_KEY" }}
            {{- end }}
          volumeMounts:
            - name: data
              mountPath: {{ .Values.server.persistence.mountPath }}
            - name: restore
              mountPath: /vault/restore
            - name: rclone-config
              mountPath: /config
              readOnly: true
          {{- with .Values.backup.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}
        {{- end }}
      containers:
        - name: server
          image: {{ template "vault.server.image" . }}
          imagePullPolicy: {{ .Values.server.image.pullPolicy }}
          {{- if .Values.server.containerSecurityContext.enabled }}
          securityContext: {{- include "common.compatibility.renderSecurityContext" (dict "secContext" .Values.server.containerSecurityContext "context" $) | nindent 12 }}
          {{- end }}
          {{- if .Values.diagnosticMode.enabled }}
          command: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.command "context" $) | nindent 12 }}
          {{- else if .Values.server.command }}
          command: {{- include "common.tplvalues.render" (dict "value" .Values.server.command "context" $) | nindent 12 }}
          {{- else if and .Values.restore.enabled .Values.backup.enabled }}
          ## Custom startup script with restore support
          command:
            - /bin/sh
            - -ec
            - |
              # Check if snapshot exists for restore
              if [ -f /vault/restore/restore.snap ]; then
                echo "=== Restoring from snapshot ==="
                # Start Vault in background
                vault server -config=/vault/config/config.hcl &
                VAULT_PID=$!
                
                # Wait for Vault to start
                sleep 5
                for i in $(seq 1 30); do
                  if vault status 2>&1 | grep -q "Seal Type"; then
                    echo "Vault is up"
                    break
                  fi
                  echo "Waiting for Vault to start... ($i/30)"
                  sleep 2
                done
                
                # Restore snapshot (works on uninitialized Vault)
                if vault operator raft snapshot restore -force /vault/restore/restore.snap 2>&1; then
                  echo "Snapshot restored successfully!"
                  rm -f /vault/restore/restore.snap
                else
                  echo "Snapshot restore failed, starting fresh"
                  rm -f /vault/restore/restore.snap
                fi
                
                # Stop background Vault
                kill $VAULT_PID 2>/dev/null || true
                wait $VAULT_PID 2>/dev/null || true
                sleep 2
              fi
              
              # Start Vault normally
              exec vault server -config=/vault/config/config.hcl {{- if .Values.server.image.debug }} -log-level=debug{{- end }}
          {{- end }}
          {{- if .Values.diagnosticMode.enabled }}
          args: {{- include "common.tplvalues.render" (dict "value" .Values.diagnosticMode.args "context" $) | nindent 12 }}
          {{- else if .Values.server.args }}
          args: {{- include "common.tplvalues.render" (dict "value" .Values.server.args "context" $) | nindent 12 }}
          {{- else if not (and .Values.restore.enabled .Values.backup.enabled) }}
          args:
            - server
            - -config=/vault/config/config.hcl
            {{- if .Values.server.image.debug }}
            - -log-level=debug
            {{- end }}
          {{- end }}
          env:
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VAULT_K8S_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VAULT_K8S_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: VAULT_ADDR
              value: {{ printf "http://127.0.0.1:%v" .Values.server.containerPorts.http | quote }}
            - name: VAULT_API_ADDR
              value: {{ printf "http://$(POD_IP):%v" .Values.server.containerPorts.http | quote }}
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: VAULT_CLUSTER_ADDR
              value: {{ printf "https://$(HOSTNAME).%s-headless:%v" ( include "vault.server.fullname" . ) .Values.server.containerPorts.internal | quote }}
            - name: HOME
              value: "/vault/home"
            {{- if .Values.server.extraEnvVars }}
            {{- include "common.tplvalues.render" (dict "value" .Values.server.extraEnvVars "context" $) | nindent 12 }}
            {{- end }}
          envFrom:
            {{- if .Values.server.extraEnvVarsCM }}
            - configMapRef:
                name: {{ include "common.tplvalues.render" (dict "value" .Values.server.extraEnvVarsCM "context" $) }}
            {{- end }}
            {{- if .Values.server.extraEnvVarsSecret }}
            - secretRef:
                name: {{ include "common.tplvalues.render" (dict "value" .Values.server.extraEnvVarsSecret "context" $) }}
            {{- end }}
          {{- if .Values.server.resources }}
          resources: {{- toYaml .Values.server.resources | nindent 12 }}
          {{- else if ne .Values.server.resourcesPreset "none" }}
          resources: {{- include "common.resources.preset" (dict "type" .Values.server.resourcesPreset) | nindent 12 }}
          {{- end }}
          ports:
            - name: http
              containerPort: {{ .Values.server.containerPorts.http }}
            - name: https-internal
              containerPort: {{ .Values.server.containerPorts.internal }}
          {{- if not .Values.diagnosticMode.enabled }}
          {{- if .Values.server.customLivenessProbe }}
          livenessProbe: {{- include "common.tplvalues.render" (dict "value" .Values.server.customLivenessProbe "context" $) | nindent 12 }}
          {{- else if .Values.server.livenessProbe.enabled }}
          livenessProbe: {{- include "common.tplvalues.render" (dict "value" (omit .Values.server.livenessProbe "enabled") "context" $) | nindent 12 }}
            exec:
              command:
                - vault
                - status
                - -tls-skip-verify
          {{- end }}
          {{- if .Values.server.customReadinessProbe }}
          readinessProbe: {{- include "common.tplvalues.render" (dict "value" .Values.server.customReadinessProbe "context" $) | nindent 12 }}
          {{- else if .Values.server.readinessProbe.enabled }}
          readinessProbe: {{- include "common.tplvalues.render" (dict "value" (omit .Values.server.readinessProbe "enabled") "context" $) | nindent 12 }}
            exec:
              command:
                - vault
                - status
                - -tls-skip-verify
          {{- end }}
          {{- if .Values.server.customStartupProbe }}
          startupProbe: {{- include "common.tplvalues.render" (dict "value" .Values.server.customStartupProbe "context" $) | nindent 12 }}
          {{- else if .Values.server.startupProbe.enabled }}
          startupProbe: {{- include "common.tplvalues.render" (dict "value" (omit .Values.server.startupProbe "enabled") "context" $) | nindent 12 }}
            exec:
              command:
                - vault
                - status
                - -tls-skip-verify
          {{- end }}
          {{- end }}
          {{- if .Values.server.lifecycleHooks }}
          lifecycle: {{- include "common.tplvalues.render" (dict "value" .Values.server.lifecycleHooks "context" $) | nindent 12 }}
          {{- else }}
          lifecycle:
            # From upstream Vault helm chart:
            # Vault container doesn't receive SIGTERM from Kubernetes
            # and after the grace period ends, Kube sends SIGKILL.  This
            # causes issues with graceful shutdowns such as deregistering itself
            # from Consul (zombie services).
            preStop:
              exec:
                command: [
                  "/bin/sh", "-ec",
                  # Adding a sleep here to give the pod eviction a
                  # chance to propagate, so requests will not be made
                  # to this pod while it's terminating
                  "sleep 5 && kill -SIGTERM $(pidof vault)",
                  ]
          {{- end }}
          volumeMounts:
            {{- if .Values.server.persistence.enabled }}
            - name: data
              mountPath: {{ .Values.server.persistence.mountPath }}
            {{- end }}
            - name: config
              mountPath: /vault/config
            - name: home
              mountPath: /vault/home
            {{- if and .Values.restore.enabled .Values.backup.enabled }}
            - name: restore
              mountPath: /vault/restore
            {{- end }}
          {{- if .Values.server.extraVolumeMounts }}
          {{- include "common.tplvalues.render" (dict "value" .Values.server.extraVolumeMounts "context" $) | nindent 12 }}
          {{- end }}
        {{- if .Values.server.sidecars }}
        {{- include "common.tplvalues.render" ( dict "value" .Values.server.sidecars "context" $) | nindent 8 }}
        {{- end }}
      volumes:
        - name: config
          configMap:
            name: {{ include "vault.server.configmapName" . }}
        - name: home
          emptyDir: {}
        {{- if and .Values.restore.enabled .Values.backup.enabled }}
        - name: restore
          emptyDir: {}
        - name: rclone-config
          configMap:
            name: {{ include "common.names.fullname" . }}-rclone-config
        {{- end }}
        {{- if and .Values.server.persistence.enabled .Values.server.persistence.existingClaim }}
        - name: data
          persistentVolumeClaim:
            claimName: {{ .Values.server.persistence.existingClaim }}
        {{- end }}
        {{- if .Values.server.extraVolumes }}
        {{- include "common.tplvalues.render" (dict "value" .Values.server.extraVolumes "context" $) | nindent 8 }}
        {{- end }}
  {{- if and .Values.server.persistence.enabled (not .Values.server.persistence.existingClaim) }}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
        annotations:
          {{- if .Values.server.persistence.annotations }}
          {{- include "common.tplvalues.render" (dict "value" .Values.server.persistence.annotations "context" $) | nindent 10 }}
          {{- end }}
          {{- if .Values.commonAnnotations }}
          {{- include "common.tplvalues.render" (dict "value" .Values.commonAnnotations "context" $) | nindent 10 }}
          {{- end }}
        {{- if .Values.commonLabels }}
        labels: {{- include "common.tplvalues.render" (dict "value" .Values.commonLabels "context" $) | nindent 10 }}
        {{- end }}
      spec:
        {{- if .Values.server.persistence.dataSource }}
        dataSource: {{- include "common.tplvalues.render" (dict "value" .Values.server.persistence.dataSource "context" $) | nindent 4 }}
        {{- end }}
        accessModes:
        {{- range .Values.server.persistence.accessModes }}
          - {{ . | quote }}
        {{- end }}
        resources:
          requests:
            storage: {{ .Values.server.persistence.size | quote }}
        {{- include "common.storage.class" (dict "persistence" .Values.server.persistence "global" .Values.global) | nindent 8 }}
        {{- if .Values.server.persistence.selector }}
        selector: {{- include "common.tplvalues.render" (dict "value" .Values.server.persistence.selector "context" $) | nindent 10 }}
        {{- end }}
    {{- if .Values.server.persistence.extraVolumeClaimTemplates }}
    {{- include "common.tplvalues.render" (dict "value" .Values.server.persistence.extraVolumeClaimTemplates "context" $) | nindent 4 }}
    {{- end }}
  {{- end }}
{{- end }}
